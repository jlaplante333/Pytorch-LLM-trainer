from IPython.display import display
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import textwrap
import torch
from trl import SFTTrainer
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments

train_dataset = load_dataset("tatsu-lab/alpaca", split="train")
print(train_dataset)



pandas_format = train_dataset.to_pandas()
display(pandas_format.head())


for index in range(3):
   print("---"*15)
print("Instruction:{}".format(textwrap.fill(pandas_format.iloc[index]["instruction"],    
       width=50)))
print("Output:{}".format(textwrap.fill(pandas_format.iloc[index]["output"],  
       width=50)))
print("Text: {}".format(textwrap.fill(pandas_format.iloc[index]["text"],  
       width=50)))

pretrained_model_name = "Salesforce/xgen-7b-8k-base"
model = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, trust_remote_code=True)

model_training_args = TrainingArguments(
       output_dir="xgen-7b-8k-base-fine-tuned",
       per_device_train_batch_size=4,
       optim="adamw_torch",
       logging_steps=80,
       learning_rate=2e-4,
       warmup_ratio=0.1,
       lr_scheduler_type="linear",
       num_train_epochs=1,
       save_strategy="epoch"
   )

pandas_format['text_length'] = pandas_format['text'].apply(len)

plt.figure(figsize=(10,6))
plt.hist(pandas_format['text_length'], bins=50, alpha=0.5, color='g')
plt.title('Distribution of Length of Text')
plt.xlabel('Length of Text')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

mask = pandas_format['text_length'] > 1024
percentage = (mask.sum() / pandas_format['text_length'].count()) * 100


print(f"The percentage of text documents with a length greater than 1024 is: {percentage}%")